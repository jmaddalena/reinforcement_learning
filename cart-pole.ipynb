{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16 # number of episodes per iteration\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a class called `Net` that inherits the `nn.Module` class. Using `nn.Sequential` we specify the structure of the neural net which is based on the size of the observation space, the desired number of nodes in the hidden layer, and use the network to predict the number of potential actions. We override the forward function using our implementation of the neural net, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define `EpisodeStep` which keeps track of the single steps within an episode and the observation and actions taken. `Episode` keeps track of the (undiscounted) return across all steps in an episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function that will iterate over all episodes of size `batch_size` and collect the rewards and number of steps from each episode.  We first initialize our variables and reset the AI Gym environment to start a fresh episode. \n",
    "\n",
    "Then the `while` loop iterates over each batch, defining the input/observations tensor `obs_v` and uses Softmax to convert all raw action scores to probabilities. We then grab these probabilities by calling the `data` attribute and converting to numpy array. We make a random choice weighted using these probabilities. We take the action by calling `env.step` and obtain our new observation, the reward for this action, and whether or not the episode ended. \n",
    "\n",
    "We keep track of the total episode reward by adding the reward from this last action and store knowledge of the observation state and action in `episode_steps`. \n",
    "\n",
    "If the episode is over, appened the results of the episode (total reward and number of steps) to `batch` nad reinitialize variables and start new episode. Only once all the batches have completed should the batch data be returned. \n",
    "\n",
    "This loop would run forever without the `break` that comes later and is determined once we consider the agent to have learned sufficiently well (takes 199 time points before stick falls). The number of batches required to get to this point is undetermined before starting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we collect training data from episodes that meets our entrance criteria: total reward must be in the 70th percentile of all episodes (the \"elite\" episodes). We get the 70th percentile of all episode rewards so far as well as the mean rewards, then keep only those above the threshold for training. Training is split into both observations (input) and actions (output). \n",
    "\n",
    "We then convert the observation training data to a 4 x n tensor, and the action data into a 2 x n tensor where n is the number of elite episodes in the batch (batch resets every time)\n",
    "\n",
    "We only store the reward boundary and mean as ways of measuring the performance of the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run our functions! We start by creating the environment, specifying the size of the observation space and the action space. We then create the network with the input layer, one hidden layer, and the output layer. We use `CrossEntropyLoss` (which is more numerically stable than using softmax and then cross entropy loss) and Adam for optimization. \n",
    "\n",
    "In our `for` loop, since we are calling `enumerate` on the generator, we generate the results of `iterate_batches` one at a time and the loop breaks (and we stop calling the potentially infinite generator) when the mean reward per episode exceeds 199. \n",
    "\n",
    "So for the given batch of episodes (recall this is the list of total episode reward and # of steps), we first call `filter_batch` to get only the elite episodes. As with all PyTorch NNs, we need to set all the gradients to zero prior to calling the network on the observation input vector, specifying the loss function. The call to `.backward()` generates the gradients of the network and `step()` updates the parameters. While the gradients are reset with each iteration, the parameters are carried through (this is how the network learns). We then write the loss and reward boundary and mean to the writer to monitor performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.684, reward_mean=16.2, reward_bound=17.5\n",
      "1: loss=0.702, reward_mean=20.6, reward_bound=20.0\n",
      "2: loss=0.692, reward_mean=23.3, reward_bound=24.0\n",
      "3: loss=0.669, reward_mean=30.8, reward_bound=34.5\n",
      "4: loss=0.642, reward_mean=41.2, reward_bound=49.0\n",
      "5: loss=0.645, reward_mean=32.2, reward_bound=39.5\n",
      "6: loss=0.634, reward_mean=40.1, reward_bound=48.5\n",
      "7: loss=0.641, reward_mean=40.1, reward_bound=48.0\n",
      "8: loss=0.609, reward_mean=44.4, reward_bound=49.0\n",
      "9: loss=0.612, reward_mean=46.3, reward_bound=54.0\n",
      "10: loss=0.625, reward_mean=46.1, reward_bound=48.0\n",
      "11: loss=0.612, reward_mean=58.9, reward_bound=61.5\n",
      "12: loss=0.590, reward_mean=50.3, reward_bound=60.0\n",
      "13: loss=0.606, reward_mean=59.0, reward_bound=58.0\n",
      "14: loss=0.580, reward_mean=63.6, reward_bound=76.0\n",
      "15: loss=0.594, reward_mean=60.1, reward_bound=65.0\n",
      "16: loss=0.577, reward_mean=53.9, reward_bound=61.0\n",
      "17: loss=0.560, reward_mean=54.1, reward_bound=60.5\n",
      "18: loss=0.590, reward_mean=78.4, reward_bound=81.5\n",
      "19: loss=0.576, reward_mean=76.4, reward_bound=75.5\n",
      "20: loss=0.558, reward_mean=72.8, reward_bound=81.0\n",
      "21: loss=0.568, reward_mean=59.4, reward_bound=68.0\n",
      "22: loss=0.563, reward_mean=93.4, reward_bound=111.5\n",
      "23: loss=0.566, reward_mean=104.1, reward_bound=134.0\n",
      "24: loss=0.550, reward_mean=80.9, reward_bound=87.0\n",
      "25: loss=0.579, reward_mean=87.6, reward_bound=104.0\n",
      "26: loss=0.571, reward_mean=101.9, reward_bound=134.5\n",
      "27: loss=0.572, reward_mean=84.2, reward_bound=94.0\n",
      "28: loss=0.583, reward_mean=82.8, reward_bound=95.5\n",
      "29: loss=0.585, reward_mean=80.7, reward_bound=81.0\n",
      "30: loss=0.572, reward_mean=88.5, reward_bound=107.0\n",
      "31: loss=0.578, reward_mean=84.4, reward_bound=98.5\n",
      "32: loss=0.544, reward_mean=95.8, reward_bound=107.0\n",
      "33: loss=0.547, reward_mean=106.6, reward_bound=137.0\n",
      "34: loss=0.547, reward_mean=151.9, reward_bound=200.0\n",
      "35: loss=0.554, reward_mean=147.8, reward_bound=200.0\n",
      "36: loss=0.559, reward_mean=162.9, reward_bound=200.0\n",
      "37: loss=0.539, reward_mean=147.4, reward_bound=196.0\n",
      "38: loss=0.552, reward_mean=147.1, reward_bound=193.0\n",
      "39: loss=0.557, reward_mean=158.2, reward_bound=200.0\n",
      "40: loss=0.542, reward_mean=149.6, reward_bound=196.5\n",
      "41: loss=0.549, reward_mean=179.3, reward_bound=200.0\n",
      "42: loss=0.542, reward_mean=174.1, reward_bound=200.0\n",
      "43: loss=0.551, reward_mean=186.4, reward_bound=200.0\n",
      "44: loss=0.546, reward_mean=185.4, reward_bound=200.0\n",
      "45: loss=0.526, reward_mean=181.4, reward_bound=200.0\n",
      "46: loss=0.536, reward_mean=188.9, reward_bound=200.0\n",
      "47: loss=0.546, reward_mean=199.8, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v) \n",
    "        loss_v = objective(action_scores_v, acts_v) \n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: It is very important to understand this very distinct difference in reinforcement learning's use of neural networks. We first use the network to make decisisons, then based on the decisions made we update the network's parameters. We then use the updated network to make decisions for a new episode, then again update the parameters. \n",
    "\n",
    "How does this relate to policy iteration, policy improvement, value iteration, etc?\n",
    "\n",
    "\n",
    "~~\n",
    "\n",
    "Running code below to see what input and output tensors look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5141e-03,  1.7276e-02, -3.7226e-02,  3.3517e-02],\n",
       "        [ 3.8597e-03,  2.1291e-01, -3.6556e-02, -2.7068e-01],\n",
       "        [ 8.1179e-03,  4.0854e-01, -4.1970e-02, -5.7466e-01],\n",
       "        [ 1.6289e-02,  2.1403e-01, -5.3463e-02, -2.9549e-01],\n",
       "        [ 2.0569e-02,  4.0987e-01, -5.9373e-02, -6.0454e-01],\n",
       "        [ 2.8766e-02,  2.1562e-01, -7.1463e-02, -3.3114e-01],\n",
       "        [ 3.3079e-02,  2.1588e-02, -7.8086e-02, -6.1817e-02],\n",
       "        [ 3.3511e-02, -1.7233e-01, -7.9323e-02,  2.0524e-01],\n",
       "        [ 3.0064e-02,  2.3829e-02, -7.5218e-02, -1.1137e-01],\n",
       "        [ 3.0541e-02, -1.7014e-01, -7.7445e-02,  1.5667e-01],\n",
       "        [ 2.7138e-02,  2.6001e-02, -7.4312e-02, -1.5941e-01],\n",
       "        [ 2.7658e-02, -1.6798e-01, -7.7500e-02,  1.0894e-01],\n",
       "        [ 2.4298e-02, -3.6191e-01, -7.5321e-02,  3.7620e-01],\n",
       "        [ 1.7060e-02, -5.5589e-01, -6.7797e-02,  6.4421e-01],\n",
       "        [ 5.9422e-03, -7.5000e-01, -5.4913e-02,  9.1480e-01],\n",
       "        [-9.0579e-03, -5.5418e-01, -3.6617e-02,  6.0538e-01],\n",
       "        [-2.0142e-02, -3.5857e-01, -2.4509e-02,  3.0139e-01],\n",
       "        [-2.7313e-02, -1.6311e-01, -1.8482e-02,  1.0767e-03],\n",
       "        [-3.0575e-02,  3.2275e-02, -1.8460e-02, -2.9738e-01],\n",
       "        [-2.9930e-02, -1.6258e-01, -2.4408e-02, -1.0576e-02],\n",
       "        [-3.3181e-02, -3.5734e-01, -2.4619e-02,  2.7431e-01],\n",
       "        [-4.0328e-02, -1.6188e-01, -1.9133e-02, -2.6037e-02],\n",
       "        [-4.3566e-02, -3.5672e-01, -1.9654e-02,  2.6055e-01],\n",
       "        [-5.0700e-02, -5.5156e-01, -1.4443e-02,  5.4697e-01],\n",
       "        [-6.1731e-02, -7.4647e-01, -3.5036e-03,  8.3507e-01],\n",
       "        [-7.6661e-02, -5.5130e-01,  1.3198e-02,  5.4128e-01],\n",
       "        [-8.7687e-02, -3.5637e-01,  2.4023e-02,  2.5279e-01],\n",
       "        [-9.4814e-02, -5.5183e-01,  2.9079e-02,  5.5295e-01],\n",
       "        [-1.0585e-01, -3.5712e-01,  4.0138e-02,  2.6957e-01],\n",
       "        [-1.1299e-01, -1.6260e-01,  4.5529e-02, -1.0190e-02],\n",
       "        [-1.1624e-01, -3.5834e-01,  4.5326e-02,  2.9650e-01],\n",
       "        [-1.2341e-01, -5.5408e-01,  5.1256e-02,  6.0313e-01],\n",
       "        [-1.3449e-01, -3.5971e-01,  6.3318e-02,  3.2702e-01],\n",
       "        [-1.4169e-01, -5.5567e-01,  6.9859e-02,  6.3898e-01],\n",
       "        [-1.5280e-01, -3.6159e-01,  8.2638e-02,  3.6909e-01],\n",
       "        [-1.6003e-01, -5.5778e-01,  9.0020e-02,  6.8664e-01],\n",
       "        [-1.7119e-01, -3.6402e-01,  1.0375e-01,  4.2360e-01],\n",
       "        [-1.7847e-01, -1.7051e-01,  1.1223e-01,  1.6535e-01],\n",
       "        [-1.8188e-01,  2.2843e-02,  1.1553e-01, -8.9931e-02],\n",
       "        [-1.8142e-01, -1.7373e-01,  1.1373e-01,  2.3685e-01],\n",
       "        [-1.8490e-01, -3.7028e-01,  1.1847e-01,  5.6313e-01],\n",
       "        [-1.9230e-01, -1.7700e-01,  1.2973e-01,  3.1000e-01],\n",
       "        [-1.9584e-01,  1.6059e-02,  1.3593e-01,  6.0880e-02],\n",
       "        [-1.9552e-01, -1.8072e-01,  1.3715e-01,  3.9317e-01],\n",
       "        [-1.9914e-01, -3.7750e-01,  1.4501e-01,  7.2576e-01],\n",
       "        [-2.0669e-01, -5.7430e-01,  1.5953e-01,  1.0603e+00],\n",
       "        [-2.1817e-01, -7.7113e-01,  1.8074e-01,  1.3985e+00],\n",
       "        [-2.3359e-01, -9.6798e-01,  2.0871e-01,  1.7419e+00],\n",
       "        [-1.3425e-02, -1.0656e-02,  2.2042e-02,  3.9624e-02],\n",
       "        [-1.3638e-02, -2.0609e-01,  2.2835e-02,  3.3918e-01],\n",
       "        [-1.7759e-02, -1.1297e-02,  2.9619e-02,  5.3784e-02],\n",
       "        [-1.7985e-02, -2.0683e-01,  3.0694e-02,  3.5566e-01],\n",
       "        [-2.2122e-02, -1.2159e-02,  3.7807e-02,  7.2814e-02],\n",
       "        [-2.2365e-02, -2.0780e-01,  3.9264e-02,  3.7718e-01],\n",
       "        [-2.6521e-02, -1.3259e-02,  4.6807e-02,  9.7133e-02],\n",
       "        [-2.6786e-02, -2.0902e-01,  4.8750e-02,  4.0421e-01],\n",
       "        [-3.0967e-02, -4.0480e-01,  5.6834e-02,  7.1185e-01],\n",
       "        [-3.9063e-02, -6.0066e-01,  7.1071e-02,  1.0219e+00],\n",
       "        [-5.1076e-02, -4.0655e-01,  9.1509e-02,  7.5232e-01],\n",
       "        [-5.9207e-02, -2.1280e-01,  1.0656e-01,  4.8978e-01],\n",
       "        [-6.3463e-02, -1.9333e-02,  1.1635e-01,  2.3249e-01],\n",
       "        [-6.3850e-02, -2.1591e-01,  1.2100e-01,  5.5949e-01],\n",
       "        [-6.8168e-02, -4.1250e-01,  1.3219e-01,  8.8771e-01],\n",
       "        [-7.6418e-02, -2.1940e-01,  1.4994e-01,  6.3933e-01],\n",
       "        [-8.0806e-02, -2.6650e-02,  1.6273e-01,  3.9737e-01],\n",
       "        [-8.1339e-02,  1.6583e-01,  1.7068e-01,  1.6009e-01],\n",
       "        [-7.8022e-02, -3.1267e-02,  1.7388e-01,  5.0139e-01],\n",
       "        [-7.8647e-02, -2.2836e-01,  1.8391e-01,  8.4343e-01],\n",
       "        [-8.3215e-02, -3.6159e-02,  2.0078e-01,  6.1376e-01],\n",
       "        [ 2.0556e-02,  4.5471e-02,  1.5814e-02,  8.2851e-04],\n",
       "        [ 2.1466e-02, -1.4987e-01,  1.5830e-02,  2.9846e-01],\n",
       "        [ 1.8468e-02,  4.5019e-02,  2.1799e-02,  1.0810e-02],\n",
       "        [ 1.9369e-02,  2.3982e-01,  2.2016e-02, -2.7492e-01],\n",
       "        [ 2.4165e-02,  4.3462e-01,  1.6517e-02, -5.6057e-01],\n",
       "        [ 3.2858e-02,  2.3927e-01,  5.3058e-03, -2.6273e-01],\n",
       "        [ 3.7643e-02,  4.3432e-01,  5.1152e-05, -5.5374e-01],\n",
       "        [ 4.6329e-02,  6.2944e-01, -1.1024e-02, -8.4641e-01],\n",
       "        [ 5.8918e-02,  4.3447e-01, -2.7952e-02, -5.5721e-01],\n",
       "        [ 6.7608e-02,  2.3975e-01, -3.9096e-02, -2.7346e-01],\n",
       "        [ 7.2403e-02,  4.3541e-01, -4.4565e-02, -5.7822e-01],\n",
       "        [ 8.1111e-02,  6.3113e-01, -5.6129e-02, -8.8460e-01],\n",
       "        [ 9.3733e-02,  4.3681e-01, -7.3821e-02, -6.1007e-01],\n",
       "        [ 1.0247e-01,  6.3288e-01, -8.6023e-02, -9.2507e-01],\n",
       "        [ 1.1513e-01,  8.2905e-01, -1.0452e-01, -1.2435e+00],\n",
       "        [ 1.3171e-01,  1.0253e+00, -1.2939e-01, -1.5670e+00],\n",
       "        [ 1.5222e-01,  1.2218e+00, -1.6073e-01, -1.8971e+00],\n",
       "        [ 1.7665e-01,  1.0287e+00, -1.9868e-01, -1.6583e+00],\n",
       "        [ 9.7003e-03, -4.4855e-02, -2.9083e-02, -3.2087e-02],\n",
       "        [ 8.8032e-03, -2.3955e-01, -2.9725e-02,  2.5128e-01],\n",
       "        [ 4.0123e-03, -4.4014e-02, -2.4699e-02, -5.0629e-02],\n",
       "        [ 3.1320e-03,  1.5145e-01, -2.5712e-02, -3.5100e-01],\n",
       "        [ 6.1611e-03,  3.4693e-01, -3.2732e-02, -6.5168e-01],\n",
       "        [ 1.3100e-02,  5.4249e-01, -4.5765e-02, -9.5449e-01],\n",
       "        [ 2.3950e-02,  3.4802e-01, -6.4855e-02, -6.7653e-01],\n",
       "        [ 3.0910e-02,  1.5385e-01, -7.8385e-02, -4.0495e-01],\n",
       "        [ 3.3987e-02, -4.0076e-02, -8.6484e-02, -1.3797e-01],\n",
       "        [ 3.3185e-02,  1.5617e-01, -8.9244e-02, -4.5664e-01],\n",
       "        [ 3.6309e-02, -3.7583e-02, -9.8376e-02, -1.9336e-01],\n",
       "        [ 3.5557e-02,  1.5880e-01, -1.0224e-01, -5.1539e-01],\n",
       "        [ 3.8733e-02,  3.5520e-01, -1.1255e-01, -8.3846e-01],\n",
       "        [ 4.5837e-02,  1.6178e-01, -1.2932e-01, -5.8319e-01],\n",
       "        [ 4.9073e-02, -3.1315e-02, -1.4098e-01, -3.3387e-01],\n",
       "        [ 4.8446e-02,  1.6550e-01, -1.4766e-01, -6.6748e-01],\n",
       "        [ 5.1756e-02, -2.7291e-02, -1.6101e-01, -4.2470e-01],\n",
       "        [ 5.1211e-02, -2.1981e-01, -1.6951e-01, -1.8679e-01],\n",
       "        [ 4.6814e-02, -2.2719e-02, -1.7324e-01, -5.2778e-01],\n",
       "        [ 4.6360e-02,  1.7436e-01, -1.8380e-01, -8.6966e-01],\n",
       "        [ 4.9847e-02,  3.7144e-01, -2.0119e-01, -1.2140e+00],\n",
       "        [ 3.4721e-02,  1.7273e-02,  4.3472e-02,  4.9853e-02],\n",
       "        [ 3.5067e-02,  2.1175e-01,  4.4469e-02, -2.2880e-01],\n",
       "        [ 3.9302e-02,  4.0621e-01,  3.9893e-02, -5.0713e-01],\n",
       "        [ 4.7426e-02,  2.1054e-01,  2.9750e-02, -2.0215e-01],\n",
       "        [ 5.1637e-02,  1.5010e-02,  2.5707e-02,  9.9765e-02],\n",
       "        [ 5.1937e-02,  2.0975e-01,  2.7702e-02, -1.8470e-01],\n",
       "        [ 5.6132e-02,  4.0447e-01,  2.4008e-02, -4.6851e-01],\n",
       "        [ 6.4221e-02,  2.0902e-01,  1.4638e-02, -1.6836e-01],\n",
       "        [ 6.8402e-02,  4.0393e-01,  1.1271e-02, -4.5639e-01],\n",
       "        [ 7.6480e-02,  2.0865e-01,  2.1429e-03, -1.6018e-01],\n",
       "        [ 8.0653e-02,  4.0374e-01, -1.0606e-03, -4.5218e-01],\n",
       "        [ 8.8728e-02,  2.0863e-01, -1.0104e-02, -1.5983e-01],\n",
       "        [ 9.2901e-02,  1.3655e-02, -1.3301e-02,  1.2964e-01],\n",
       "        [ 9.3174e-02,  2.0896e-01, -1.0708e-02, -1.6721e-01],\n",
       "        [ 9.7353e-02,  1.3998e-02, -1.4052e-02,  1.2208e-01],\n",
       "        [ 9.7633e-02, -1.8092e-01, -1.1611e-02,  4.1030e-01],\n",
       "        [ 9.4014e-02, -3.7588e-01, -3.4047e-03,  6.9930e-01],\n",
       "        [ 8.6497e-02, -5.7095e-01,  1.0581e-02,  9.9091e-01],\n",
       "        [ 7.5078e-02, -7.6621e-01,  3.0399e-02,  1.2869e+00],\n",
       "        [ 5.9754e-02, -9.6171e-01,  5.6137e-02,  1.5889e+00],\n",
       "        [ 4.0520e-02, -1.1574e+00,  8.7916e-02,  1.8986e+00],\n",
       "        [ 1.7371e-02, -9.6338e-01,  1.2589e-01,  1.6344e+00],\n",
       "        [-1.8971e-03, -7.6994e-01,  1.5858e-01,  1.3835e+00],\n",
       "        [-1.7296e-02, -9.6665e-01,  1.8625e-01,  1.7213e+00],\n",
       "        [-1.2396e-02, -4.1386e-02,  8.7687e-03, -1.5029e-02],\n",
       "        [-1.3224e-02, -2.3663e-01,  8.4681e-03,  2.8041e-01],\n",
       "        [-1.7956e-02, -4.1632e-02,  1.4076e-02, -9.5930e-03],\n",
       "        [-1.8789e-02,  1.5328e-01,  1.3884e-02, -2.9780e-01],\n",
       "        [-1.5723e-02,  3.4821e-01,  7.9284e-03, -5.8607e-01],\n",
       "        [-8.7592e-03,  1.5297e-01, -3.7931e-03, -2.9090e-01],\n",
       "        [-5.6997e-03, -4.2093e-02, -9.6112e-03,  5.8051e-04],\n",
       "        [-6.5416e-03, -2.3708e-01, -9.5996e-03,  2.9022e-01],\n",
       "        [-1.1283e-02, -4.3206e-01, -3.7953e-03,  5.7986e-01],\n",
       "        [-1.9924e-02, -6.2713e-01,  7.8018e-03,  8.7134e-01],\n",
       "        [-3.2467e-02, -8.2236e-01,  2.5229e-02,  1.1665e+00],\n",
       "        [-4.8914e-02, -6.2757e-01,  4.8558e-02,  8.8180e-01],\n",
       "        [-6.1465e-02, -8.2332e-01,  6.6194e-02,  1.1893e+00],\n",
       "        [-7.7932e-02, -6.2911e-01,  8.9981e-02,  9.1812e-01],\n",
       "        [-9.0514e-02, -8.2533e-01,  1.0834e-01,  1.2377e+00],\n",
       "        [-1.0702e-01, -1.0217e+00,  1.3310e-01,  1.5622e+00],\n",
       "        [-1.2745e-01, -1.2181e+00,  1.6434e-01,  1.8933e+00],\n",
       "        [-1.5182e-01, -1.0251e+00,  2.0221e-01,  1.6558e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "batch = []\n",
    "episode_reward = 0.0\n",
    "episode_steps = []\n",
    "obs = env.reset()\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "while len(batch) < batch_size:\n",
    "    obs_v = torch.FloatTensor([obs])\n",
    "    act_probs_v = sm(net(obs_v))\n",
    "    act_probs = act_probs_v.data.numpy()[0]\n",
    "    action = np.random.choice(len(act_probs), p=act_probs)\n",
    "    next_obs, reward, is_done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "    if is_done:\n",
    "        batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = []\n",
    "        next_obs = env.reset()\n",
    "    obs = next_obs\n",
    "    \n",
    "obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "\n",
    "obs_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  0,  1,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,\n",
       "         1,  1,  1,  1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  1,\n",
       "         1,  0,  0,  1,  0,  1,  0,  1,  1,  1,  0,  0,  1,  1,\n",
       "         0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  1,  0,  0,\n",
       "         0,  1,  1,  1,  0,  0,  1,  1,  1,  0,  0,  1,  1,  0,\n",
       "         1,  1,  1,  0,  1,  1,  0,  0,  1,  1,  0,  1,  1,  1,\n",
       "         1,  0,  1,  0,  1,  1,  1,  1,  0,  0,  0,  1,  0,  1,\n",
       "         1,  0,  0,  1,  0,  0,  1,  1,  1,  0,  1,  1,  0,  0,\n",
       "         1,  1,  0,  1,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "         0,  0,  1,  1,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,\n",
       "         0,  0,  1,  0,  1,  0,  0,  0,  1,  1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
